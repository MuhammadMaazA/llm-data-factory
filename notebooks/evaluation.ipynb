{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f676f4b",
   "metadata": {},
   "source": [
    "# LLM Data Factory - Model Evaluation\n",
    "\n",
    "This notebook evaluates the performance of our fine-tuned Phi-3-mini student model on customer support ticket classification.\n",
    "\n",
    "## Evaluation Overview\n",
    "\n",
    "We will:\n",
    "1. Load the fine-tuned model\n",
    "2. Load the test dataset\n",
    "3. Generate predictions\n",
    "4. Analyze performance metrics\n",
    "5. Create visualizations\n",
    "6. Compare with baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547d711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the parent directory to path for imports\n",
    "sys.path.append(str(Path().parent))\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\" Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb248468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the test dataset\n",
    "test_data_path = \"../data/test_data.json\"\n",
    "\n",
    "try:\n",
    "    with open(test_data_path, 'r') as f:\n",
    "        test_data = json.load(f)\n",
    "    \n",
    "    print(f\" Loaded {len(test_data)} test samples\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    test_df = pd.DataFrame(test_data)\n",
    "    \n",
    "    # Display basic info about the test set\n",
    "    print(\"\\n Test Dataset Overview:\")\n",
    "    print(f\"Total samples: {len(test_df)}\")\n",
    "    print(f\"Categories: {test_df['category'].unique()}\")\n",
    "    print(f\"\\nCategory distribution:\")\n",
    "    print(test_df['category'].value_counts())\n",
    "    \n",
    "    # Display first few examples\n",
    "    print(f\"\\n Sample test tickets:\")\n",
    "    for i, row in test_df.head(3).iterrows():\n",
    "        print(f\"\\n{i+1}. Category: {row['category']}\")\n",
    "        print(f\"   Message: {row['customer_message'][:100]}...\")\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"Test data file not found: {test_data_path}\")\n",
    "    print(\"Please ensure you have created the test dataset\")\n",
    "except Exception as e:\n",
    "    print(f\" Error loading test data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43715b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "try:\n",
    "    from app.inference import load_classifier, predict_ticket_category\n",
    "    \n",
    "    print(\"üîÑ Loading the fine-tuned model...\")\n",
    "    classifier = load_classifier()\n",
    "    \n",
    "    if classifier is not None:\n",
    "        print(\"‚úÖ Model loaded successfully!\")\n",
    "        \n",
    "        # Test with a sample prediction\n",
    "        test_message = \"The app keeps crashing when I try to save my work. This is very urgent!\"\n",
    "        result = predict_ticket_category(classifier, test_message)\n",
    "        \n",
    "        print(f\"\\nüß™ Test prediction:\")\n",
    "        print(f\"Message: {test_message}\")\n",
    "        print(f\"Predicted: {result['predicted_category']}\")\n",
    "        print(f\"Confidence: {result['confidence']:.3f}\")\n",
    "        print(f\"All probabilities: {result['probabilities']}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå Failed to load model\")\n",
    "        print(\"This might be because the model hasn't been trained yet.\")\n",
    "        print(\"Please run: python scripts/02_finetune_student_model.py\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    print(\"Make sure you have trained the model first.\")\n",
    "    classifier = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19211243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for all test samples\n",
    "if classifier is not None and 'test_df' in locals():\n",
    "    print(\"üîÑ Generating predictions for all test samples...\")\n",
    "    \n",
    "    predictions = []\n",
    "    confidences = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    for idx, row in test_df.iterrows():\n",
    "        try:\n",
    "            result = predict_ticket_category(classifier, row['customer_message'])\n",
    "            predictions.append(result['predicted_category'])\n",
    "            confidences.append(result['confidence'])\n",
    "            all_probabilities.append(result['probabilities'])\n",
    "            \n",
    "            if (idx + 1) % 10 == 0:\n",
    "                print(f\"Processed {idx + 1}/{len(test_df)} samples...\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing sample {idx}: {e}\")\n",
    "            predictions.append(\"Unknown\")\n",
    "            confidences.append(0.0)\n",
    "            all_probabilities.append({})\n",
    "    \n",
    "    # Add predictions to dataframe\n",
    "    test_df['predicted_category'] = predictions\n",
    "    test_df['confidence'] = confidences\n",
    "    test_df['probabilities'] = all_probabilities\n",
    "    \n",
    "    print(f\"‚úÖ Generated predictions for {len(test_df)} samples\")\n",
    "    \n",
    "    # Display some example predictions\n",
    "    print(f\"\\nüìù Sample predictions:\")\n",
    "    for i, row in test_df.head(5).iterrows():\n",
    "        correct = \"‚úÖ\" if row['category'] == row['predicted_category'] else \"‚ùå\"\n",
    "        print(f\"{correct} True: {row['category']} | Predicted: {row['predicted_category']} | Confidence: {row['confidence']:.3f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping predictions - model not loaded or test data not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e2927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and display performance metrics\n",
    "if 'predicted_category' in test_df.columns:\n",
    "    print(\"üìä Performance Metrics\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Get true and predicted labels\n",
    "    y_true = test_df['category'].tolist()\n",
    "    y_pred = test_df['predicted_category'].tolist()\n",
    "    \n",
    "    # Overall accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    print(f\"üéØ Overall Accuracy: {accuracy:.3f} ({accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(f\"\\nüìã Detailed Classification Report:\")\n",
    "    print(\"-\" * 40)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    \n",
    "    # Print formatted report\n",
    "    for category, metrics in report.items():\n",
    "        if category not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "            print(f\"  Recall:    {metrics['recall']:.3f}\")\n",
    "            print(f\"  F1-Score:  {metrics['f1-score']:.3f}\")\n",
    "            print(f\"  Support:   {metrics['support']}\")\n",
    "    \n",
    "    # Macro and weighted averages\n",
    "    print(f\"\\nüìà Average Metrics:\")\n",
    "    print(f\"Macro Average F1:    {report['macro avg']['f1-score']:.3f}\")\n",
    "    print(f\"Weighted Average F1: {report['weighted avg']['f1-score']:.3f}\")\n",
    "    \n",
    "    # Per-category accuracy\n",
    "    print(f\"\\nüéØ Per-Category Accuracy:\")\n",
    "    for category in test_df['category'].unique():\n",
    "        mask = test_df['category'] == category\n",
    "        cat_accuracy = (test_df[mask]['category'] == test_df[mask]['predicted_category']).mean()\n",
    "        cat_count = mask.sum()\n",
    "        print(f\"{category}: {cat_accuracy:.3f} ({cat_count} samples)\")\n",
    "    \n",
    "    # Confidence analysis\n",
    "    print(f\"\\nüîç Confidence Analysis:\")\n",
    "    print(f\"Average Confidence: {test_df['confidence'].mean():.3f}\")\n",
    "    print(f\"Min Confidence: {test_df['confidence'].min():.3f}\")\n",
    "    print(f\"Max Confidence: {test_df['confidence'].max():.3f}\")\n",
    "    \n",
    "    # Correct vs incorrect predictions confidence\n",
    "    correct_mask = test_df['category'] == test_df['predicted_category']\n",
    "    correct_conf = test_df[correct_mask]['confidence'].mean()\n",
    "    incorrect_conf = test_df[~correct_mask]['confidence'].mean()\n",
    "    \n",
    "    print(f\"Avg Confidence (Correct): {correct_conf:.3f}\")\n",
    "    print(f\"Avg Confidence (Incorrect): {incorrect_conf:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping metrics calculation - predictions not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203bb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrix visualization\n",
    "if 'predicted_category' in test_df.columns:\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    categories = sorted(test_df['category'].unique())\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Model Evaluation Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Confusion Matrix\n",
    "    ax1 = axes[0, 0]\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=categories, yticklabels=categories, ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix')\n",
    "    ax1.set_xlabel('Predicted')\n",
    "    ax1.set_ylabel('Actual')\n",
    "    \n",
    "    # 2. Category Distribution\n",
    "    ax2 = axes[0, 1]\n",
    "    category_counts = test_df['category'].value_counts()\n",
    "    ax2.bar(category_counts.index, category_counts.values)\n",
    "    ax2.set_title('Test Set Category Distribution')\n",
    "    ax2.set_xlabel('Category')\n",
    "    ax2.set_ylabel('Count')\n",
    "    ax2.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 3. Confidence Distribution\n",
    "    ax3 = axes[1, 0]\n",
    "    correct_conf = test_df[test_df['category'] == test_df['predicted_category']]['confidence']\n",
    "    incorrect_conf = test_df[test_df['category'] != test_df['predicted_category']]['confidence']\n",
    "    \n",
    "    ax3.hist(correct_conf, alpha=0.7, label='Correct', bins=20, color='green')\n",
    "    ax3.hist(incorrect_conf, alpha=0.7, label='Incorrect', bins=20, color='red')\n",
    "    ax3.set_title('Confidence Distribution')\n",
    "    ax3.set_xlabel('Confidence Score')\n",
    "    ax3.set_ylabel('Frequency')\n",
    "    ax3.legend()\n",
    "    \n",
    "    # 4. Per-category F1 scores\n",
    "    ax4 = axes[1, 1]\n",
    "    f1_scores = []\n",
    "    for category in categories:\n",
    "        cat_report = classification_report(y_true, y_pred, output_dict=True)\n",
    "        f1_scores.append(cat_report[category]['f1-score'])\n",
    "    \n",
    "    bars = ax4.bar(categories, f1_scores)\n",
    "    ax4.set_title('F1-Score by Category')\n",
    "    ax4.set_xlabel('Category')\n",
    "    ax4.set_ylabel('F1-Score')\n",
    "    ax4.tick_params(axis='x', rotation=45)\n",
    "    ax4.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, score in zip(bars, f1_scores):\n",
    "        ax4.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, \n",
    "                f'{score:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üìä Visualizations created successfully!\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping visualizations - predictions not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac8e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results and generate final report\n",
    "if 'predicted_category' in test_df.columns:\n",
    "    # Save detailed results\n",
    "    results_path = \"../results\"\n",
    "    os.makedirs(results_path, exist_ok=True)\n",
    "    \n",
    "    # Save predictions with details\n",
    "    results_df = test_df[['ticket_id', 'customer_message', 'category', \n",
    "                         'predicted_category', 'confidence']].copy()\n",
    "    results_df['correct'] = results_df['category'] == results_df['predicted_category']\n",
    "    \n",
    "    results_df.to_csv(f\"{results_path}/detailed_predictions.csv\", index=False)\n",
    "    print(f\"‚úÖ Saved detailed predictions to {results_path}/detailed_predictions.csv\")\n",
    "    \n",
    "    # Save summary metrics\n",
    "    summary_metrics = {\n",
    "        \"overall_accuracy\": float(accuracy),\n",
    "        \"total_samples\": len(test_df),\n",
    "        \"correct_predictions\": int(correct_mask.sum()),\n",
    "        \"average_confidence\": float(test_df['confidence'].mean()),\n",
    "        \"per_category_metrics\": {},\n",
    "        \"confusion_matrix\": cm.tolist(),\n",
    "        \"categories\": categories\n",
    "    }\n",
    "    \n",
    "    # Add per-category metrics\n",
    "    for category in categories:\n",
    "        cat_metrics = report[category]\n",
    "        summary_metrics[\"per_category_metrics\"][category] = {\n",
    "            \"precision\": float(cat_metrics['precision']),\n",
    "            \"recall\": float(cat_metrics['recall']),\n",
    "            \"f1_score\": float(cat_metrics['f1-score']),\n",
    "            \"support\": int(cat_metrics['support'])\n",
    "        }\n",
    "    \n",
    "    # Save summary metrics as JSON\n",
    "    with open(f\"{results_path}/evaluation_summary.json\", 'w') as f:\n",
    "        json.dump(summary_metrics, f, indent=2)\n",
    "    print(f\"‚úÖ Saved evaluation summary to {results_path}/evaluation_summary.json\")\n",
    "    \n",
    "    # Generate final report\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéâ FINAL EVALUATION REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(f\"\\nüìà Overall Performance:\")\n",
    "    print(f\"   ‚Ä¢ Accuracy: {accuracy:.1%}\")\n",
    "    print(f\"   ‚Ä¢ Total Samples: {len(test_df)}\")\n",
    "    print(f\"   ‚Ä¢ Correct Predictions: {correct_mask.sum()}\")\n",
    "    print(f\"   ‚Ä¢ Average Confidence: {test_df['confidence'].mean():.3f}\")\n",
    "    \n",
    "    print(f\"\\nüèÜ Best Performing Category:\")\n",
    "    best_category = max(categories, key=lambda x: report[x]['f1-score'])\n",
    "    best_f1 = report[best_category]['f1-score']\n",
    "    print(f\"   ‚Ä¢ {best_category}: F1-Score = {best_f1:.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è Category Needing Improvement:\")\n",
    "    worst_category = min(categories, key=lambda x: report[x]['f1-score'])\n",
    "    worst_f1 = report[worst_category]['f1-score']\n",
    "    print(f\"   ‚Ä¢ {worst_category}: F1-Score = {worst_f1:.3f}\")\n",
    "    \n",
    "    print(f\"\\nüí° Key Insights:\")\n",
    "    if correct_conf > incorrect_conf:\n",
    "        print(f\"   ‚Ä¢ Model shows good confidence calibration\")\n",
    "        print(f\"     (Correct: {correct_conf:.3f} vs Incorrect: {incorrect_conf:.3f})\")\n",
    "    \n",
    "    if accuracy > 0.8:\n",
    "        print(f\"   ‚Ä¢ Excellent performance! Model ready for production\")\n",
    "    elif accuracy > 0.7:\n",
    "        print(f\"   ‚Ä¢ Good performance, consider additional tuning\")\n",
    "    else:\n",
    "        print(f\"   ‚Ä¢ Model needs improvement, review training data/process\")\n",
    "    \n",
    "    print(\"\\nüéØ Next Steps:\")\n",
    "    print(\"   ‚Ä¢ Deploy model to production if performance is satisfactory\")\n",
    "    print(\"   ‚Ä¢ Consider collecting more training data for underperforming categories\")\n",
    "    print(\"   ‚Ä¢ Monitor model performance in real-world scenarios\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚è≠Ô∏è Skipping final report - predictions not available\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
